{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61fc2a1a-843c-460f-84d3-8e05e04375eb",
   "metadata": {},
   "source": [
    "# Solution Evaluation\n",
    "\n",
    "This notebook give an overview about how an AI component candidate will be evaluated.  Even if it does not compute all metrics, it give a precise idea , about how to build an compatible AI component, and how it is used for score generation.\n",
    "\n",
    "## Objective\n",
    "\n",
    "The task is to build a AI component whose main task whose main objective is to predict the welding state from a given image.\n",
    "\n",
    "### Inputs to the AI component\n",
    "The AI component shall takes as input : \n",
    "- A list of numpy arrays representing the list of input images to process . \n",
    "- A list  of dictionnary containing a meta-description of the image.\n",
    "\n",
    "### The outputs of the AI component  \n",
    "It shall return a dictionnary with four keys {predictions , probabilities, OOD_score, explainabilities}\n",
    "    \n",
    "First key is required    \n",
    "- *predictions*:  The list of predicted welding state. The welding state can have three possible values: [OK, KO, UNKNOWN]\n",
    "    \n",
    "The following keys are not mandatory, but will greatly participate to the improvement of the quality score for the developed AI component if present:\n",
    "\n",
    "- *probabilities*:  The list of associated probabilities for each images [$P_{KO}$, $P_{OK}$, $P_{UNKNWON}$]  where $\\sum_{i \\in \\{\\text{OK, KO, UNKNOWN}\\}} P_i = 1$.\n",
    "\n",
    "- *OOD_scores*: The list OOD score predicted by the AI component of for each images. This score X is a real positive. If $0\\leq X < 1$ the image is considered as *In-Domain*, if $X >1$ the image is considered as *Out-of-Domain (OoD)*.\n",
    "\n",
    "- *explainabilities*: The list of explainabilities for each input images. An explainability is an intensity matrix ( matrix with values between 0 and 1) with same size of the image tensor, that represents the importance of each pixel in the model prediction\n",
    "\n",
    "### Evaluation criteria\n",
    "From the predictions made by the developed AI component, we will compute a set of different evaluation criteria as discussed below:\n",
    "\n",
    "- **Operationnal metrics**: Measure the gain brought the AI component compared to a human only qualification process. This metrics is based on the confusion matrix and penalize strongly false negative predictions. \n",
    "\n",
    "- **Uncertainty metrics**: Measure the ability of the AI component to produce a calibrated prediction confidence indicator expressing risk of model errors.\n",
    "\n",
    "- **Robustness metrics**: Measure the ability of the AI component to have invariant output is images have slight perturbations (blut, luminosity, rotation, translation)\n",
    "\n",
    "- **Monitoring metrics**: Measure the ability of the AI component to detect if an input image is ood, and gives the appropriate output ->Unknown\n",
    "\n",
    "- **Explainability metrics**: Measure the ability of the AI component to give appropriate explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eefd4e9",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "Install the dependencies if it is not already done. For more information look at the [readme](../README.md) file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7417ec9",
   "metadata": {},
   "source": [
    "##### For development on Local Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab045ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Install a virtual environment\n",
    "# Option 1:  using conda (recommended)\n",
    "!conda create -n venv python=3.12\n",
    "!conda activate venv\n",
    "!pip install torch==2.6.0\n",
    "\n",
    "# Option 2: using virtualenv\n",
    "# !pip install virtualenv\n",
    "# !virtualenv -p /usr/bin/python3.12 venv\n",
    "# !source venv_lips/bin/activate\n",
    "\n",
    "### Install the welding challenge package\n",
    "# Option 1: Get the last version from Pypi\n",
    "# !pip install 'challenge_welding'\n",
    "\n",
    "# Option 2: Get the last version from github repository\n",
    "# !git clone https://github.com/XX\n",
    "# !pip install -U ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd664cab",
   "metadata": {},
   "source": [
    "##### For Google Colab Users\n",
    "You could also use a GPU device from Runtime > Change runtime type and by selecting T4 GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f357818a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Install the welding challenge package\n",
    "# Option 1: Get the last version of LIPS framework from PyPI (Recommended)\n",
    "!pip install 'XX'\n",
    "!pip install torch==2.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa63e28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Get the last version from github repository\n",
    "!git clone https://github.com/XX\n",
    "!pip install -U .\n",
    "!pip install torch==2.6.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca469586",
   "metadata": {},
   "source": [
    "Attention: You may restart the session after this installation, in order that the changes be effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2828e1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the starting kit\n",
    "!git clone https://github.com/confianceai/Challenge-Welding-Starter-Kit.git\n",
    "# and change the directory to the starting kit to be able to run correctly this notebook\n",
    "import os\n",
    "os.chdir(\"Challenge-Welding-Starter-Kit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869b77c0",
   "metadata": {},
   "source": [
    "## Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ef937f7-c985-4c30-bef2-f7e4dd1c143c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# sys.path.insert(0, \"..\") # Uncomment this line For local tests without pkg installation, to make challenge_welding module visible \n",
    "from challenge_welding.user_interface import ChallengeUI\n",
    "from challenge_welding.Evaluation_tools import EvaluationPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be48b7c-74d4-4e10-b338-bf7e4ba21f25",
   "metadata": {},
   "source": [
    "# Build your AI component\n",
    "An AI component shall be a buildable python package . Thus, it is a folder that shall have at least the following files and folders :\n",
    "   ```\n",
    "    /\n",
    "    setup.py\n",
    "    requirements.txt\n",
    "    challenge_solution/\n",
    "        AIcomponent.py\n",
    "        __init__.py\n",
    " ```\n",
    "\n",
    "Only those files will be used by the evaluation pipeline to test your AI component. The names of files and folders shall not be changed.\n",
    "The most important file is the AIcomponent.py file that is the interface of your AI component. Only this interface will be used by the evaluation pipeline to interact with your component. That is why this file require some strict named methods and class to be present. It shall follow this abstract class [Aicomponent interface](absAIcomponent.py)\n",
    "\n",
    "In this starter-kit , we provided an example of such AI component, that is evaluated here. This example AI component has been built just to show what is a correct a AI component architecture. It has no good performance in kind of quality predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd53dbf4-03c4-40d3-9a69-c89d6b565817",
   "metadata": {},
   "source": [
    "# Create an evaluation pipeline\n",
    "\n",
    "An evaluation pipeline take an AI component (the solution to test) and evaluate it by generating differents metrics and scores. In this notebooks we generate only operationnal metrics and uncertainty scores. \n",
    "\n",
    "An evaluation pipeline :\n",
    "- Install your AI component as a python package \n",
    "- Load the AI component of the solution you want to test\n",
    "- Apply inference on this AI component on one or many evaluation datasets. Each inference process on a dataset generate as output a dataframe( stored as a parquet file) containing evaluation dataset metadata extended with prediction results.\n",
    "- Apply metrics computation functions that takes only inference_results as parquet files to generate output metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e327bf-490d-4425-95c9-cba6940244b7",
   "metadata": {},
   "source": [
    "## Init the pipeline\n",
    "To set the evaluation pipeline, you should instantiate the `EvaluationPipeline` class with the following parameters:\n",
    "- `proposed_solution_path`: Path to the folder containing the AI component\n",
    "- `meta_root_path`: Path where pipeline results will be stored (inference results, and computed metrics)\n",
    "- `cache_strategy`: Could be `local` or `remote`. If set on `local`, all image used for evaluation , will be locally stored in a cache directory. Else, image will be loaded directly from downloding\n",
    "- `cache_dir`: A directory where the cache should be stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cb0681b-af28-40cc-bc93-87c539ec1f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path of AI component to test\n",
    "AI_comp_path = os.path.join(\"..\", \"reference-solutions\", \"Solution-1\")\n",
    "\n",
    "# Initialize test pipeline\n",
    "myPipeline=EvaluationPipeline(proposed_solution_path=AI_comp_path,\n",
    "                              meta_root_path=\"starter_kit_pipeline_results\",\n",
    "                              cache_strategy=\"local\",\n",
    "                              cache_dir=\"evaluation_cache\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fddaef-6902-4025-b785-2e3ee76aa7a9",
   "metadata": {},
   "source": [
    "## Load your AI component into the evaluation environnement\n",
    "\n",
    "The `load_proposed_solution()` method below is divided into two main tasks:\n",
    "- Install the python package of your AI component --> ( execute the commande pip install AI_comp_path)\n",
    "- Call the load_model() method of your AIcomponent interface\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75924591-27d3-47b3-b6e1-7bf423321581",
   "metadata": {},
   "outputs": [],
   "source": [
    "myPipeline.load_proposed_solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd65bfb-9bf1-487b-a752-b82df711d9a6",
   "metadata": {},
   "source": [
    "## Load an evaluation dataset metadescription\n",
    "\n",
    "In the next cell, we load the metadata of the evaluation dataset we want to use to evaluate our AI component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a83a8a-c53d-42cf-8bda-feccbd9fba45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this example we will choose a small dataset\n",
    "ds_name=\"example_mini_dataset\"\n",
    "\n",
    "# Load all metadata of your dataset as a pandas dataframe, (you can point to a local cache metafile instead of original one pointing on remote repository)\n",
    "my_challenge_UI=ChallengeUI()\n",
    "evaluation_ds_meta_df=my_challenge_UI.get_ds_metadata_dataframe(ds_name)\n",
    "\n",
    "display(evaluation_ds_meta_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e70108-edb7-4de7-85ab-8048a737aaa0",
   "metadata": {},
   "source": [
    "##  Perform inference on an evaluation dataset\n",
    "Once the AI component is loaded to the pipeline, we use `perform_grouped_inference` function to make inference on a dataset. For demonstration, we use the `example_mini_dataset` as evaluation dataset. This function takes the following arguments:\n",
    "- `evaluation_dataset`: dataframe containing metadescription of your evaluation ds\n",
    "- `results_inference_path`: path to file that will contains inference_results\n",
    "- `batch_size`: You can group inference by batch if it is required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02df7d85-68dc-4f3f-8456-91758c06f5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df=myPipeline.perform_grouped_inference(evaluation_dataset=evaluation_ds_meta_df,\n",
    "                                               results_inference_path=myPipeline.meta_root_path+\"/res_inference.parquet\",\n",
    "                                               batch_size=150\n",
    "                                              ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e12ffb1-1182-4e6d-b23c-578231d12199",
   "metadata": {},
   "source": [
    "The inference results are added as additional columns to the dataframe. These new columns are added :\n",
    "- `predicted_state` imported from `predicitions` key of the dictionary returned by the predict function of your AI component \n",
    "- `scores KO` imported from `probabilities` key of the dictionary returned by the predict function of your AI component \n",
    "- `scores OK` imported from `probabilities` key of the dictionary returned by the predict function of your AI component \n",
    "- `score OOD` imported from `OOD scores` key of the dictionary returned by the predict function of your AI component \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c18ec49-b6ce-413e-b209-b75634a44193",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78948c8f-361c-475f-ad3a-729022f181e6",
   "metadata": {},
   "source": [
    "## Compute operationnal metrics\n",
    "\n",
    "The first criterion to evaluate your component quality will be the operationnal cost. The objective is to compare the cost of using your AI component versus\n",
    "the cost of using only humans to process images from the evaluation dataset. The import scores to maximimze is \"gain in euros\" and inference_time\n",
    "\n",
    "As the example AI component we provided in this starter-kit has not been designed to be performant, here the gain_score is very bad\n",
    "\n",
    "The below method compute these two metrics, results are display as output, and in results folder defined at this pipeline initilization too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfc2c1b-0f24-4e50-80fd-8afb26187fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute operationnal metrics\n",
    "myPipeline.compute_operationnal_metrics(AIcomp_name=\"sol_0\", # This name is only used for the name of result files\n",
    "                                        res_inference_path=myPipeline.meta_root_path+\"/res_inference.parquet\" # inference_results file\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880c4cd0-5234-4948-bdd3-bd0e66e4b877",
   "metadata": {},
   "source": [
    "## Compute uncertainty metrics\n",
    "\n",
    "Same thing with uncertainty metrics\n",
    "\n",
    "To detail\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c516ad12-2af2-45aa-a9ed-69c69c38d269",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df,final_results=myPipeline.compute_uncertainty_metrics(res_inference_path=myPipeline.meta_root_path+\"/res_inference.parquet\",\n",
    "                                                            AIcomp_name=\"sol_0\"\n",
    "                                                            )      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_confiance_fresh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
