{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61fc2a1a-843c-460f-84d3-8e05e04375eb",
   "metadata": {},
   "source": [
    "# Solution Evaluation\n",
    "\n",
    "This notebook give an overview about how an AI component candidate will be evaluated.  Even if it does not compute all metrics, it give a precise idea , about how to build an compatible AI component, and how it is used for score generation.\n",
    "\n",
    "# Objective\n",
    "\n",
    "The task is to build a AI component whose main task whose main objective is to predict the welding state from a given image.\n",
    "If the image is in the operationnal domain :\n",
    "\n",
    "The AI component shall takes as input : \n",
    " - A list of numpy arrays representing the list of input images to process . \n",
    "- A list  of dictionnary containing a metadescription of the image.\n",
    "    \n",
    "It shall return a dictionnary with four keys {predictions , probabilities, OOD_score, explainabilities}\n",
    "    \n",
    "First key is required    \n",
    "- predictions:  The list of predicted welding state. The welding state can have three possible values: [OK, KO, UNKNOWN]\n",
    "    \n",
    "Others keys are not required but will greatly improve the quality score of the component if present.\n",
    "\n",
    "- probabilities:  The list of associated probabilities for each images [ proba KO, proba OK, proba UNKNWON] (sum of proba =1)\n",
    "- OOD_scores : The list OOD score predicted by the AI component of for each images. This score X is a real positive. If 0<= X <1 the image is considered as in Domain, if X >1 the image is considered as OOD.\n",
    "- explainabilities : The list of explainabilities for each input images. An explainability is an intensity matrix ( matrix with values between 0 and 1) with same size of the image tensor, that represents the importance of each pixel in the model prediction\n",
    "\n",
    "## Evaluation criterions\n",
    "\n",
    "Operationnal metrics : Measure the gain brought the AI component compared to a human only qualification process. This metrics is based on the confusion matrix and penalize strongly false negative predictions. \n",
    "\n",
    "Uncertainty metrics: Measure the ability of the AI component to produce a calibrated prediction confidence indicator expressing risk of model errors.\n",
    "\n",
    "Robustness metrics : Measure the ability of the AI component to have invariant output is images have slight perturbations (blut, luminosity, rotation, translation)\n",
    "\n",
    "Monitoring metrics: Measure the ability of the AI component to detect if an input image is ood, and gives the appropriate output ->Unknown\n",
    "\n",
    "Explainability metrics : Measure the ability of the AI component to give appropriate explanations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ef937f7-c985-4c30-bef2-f7e4dd1c143c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# sys.path.insert(0, \"..\") # Uncomment this line For local tests without pkg installation, to make challenge_welding module visible \n",
    "from challenge_welding.user_interface import ChallengeUI\n",
    "from challenge_welding.Evaluation_tools import EvaluationPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be48b7c-74d4-4e10-b338-bf7e4ba21f25",
   "metadata": {},
   "source": [
    "# Build your AI component\n",
    "An AI component shall be a buildable python package . Thus, it is a folder that shall have at least the following files and folders :\n",
    "   ```\n",
    "    /\n",
    "    setup.py\n",
    "    requirements.txt\n",
    "    challenge_solution/\n",
    "        AIcomponent.py\n",
    "        __init__.py\n",
    " ```\n",
    "\n",
    "Only those files will be used by the evaluation pipeline to test your AI component. The names of files and folders shall not be changed.\n",
    "The most important file is the AIcomponent.py file that is the interface of your AI component. Only this interface will be used by the evaluation pipeline to interact with your component. That is why this file require some strict named methods and class to be present. It shall follow this abstract class [AIComponent interface](../absAIComponent.py)\n",
    "\n",
    "In this starter-kit , we provided an example of such AI component, that is evaluated here. This example AI component has been built just to show what is a correct a AI component architecture. It has no good performance in kind of quality predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd53dbf4-03c4-40d3-9a69-c89d6b565817",
   "metadata": {},
   "source": [
    "# Create an evaluation pipeline\n",
    "\n",
    "An evaluation pipeline take an AI component (the solution to test) and evaluate it by generating differents metrics and scores. In this notebooks we generate only operationnal metrics and uncertainty scores. \n",
    "\n",
    "An evaluation pipeline :\n",
    "- Install your AI component as a python package \n",
    "- Load the AI component of the solution you want to test\n",
    "- Apply inference on this AI component on one or many evaluation datasets. Each inference process on a dataset generate as output a dataframe( stored as a parquet file) containing evaluation dataset metadata extended with prediction results.\n",
    "- Apply metrics computation functions that takes only inference_results as parquet files to generate output metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e327bf-490d-4425-95c9-cba6940244b7",
   "metadata": {},
   "source": [
    "## Init the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cb0681b-af28-40cc-bc93-87c539ec1f6c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'EvaluationPipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m AI_comp_path= \u001b[33m\"\u001b[39m\u001b[33m..\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mreference-solutions\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mSolution-1\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Initialize test pipeline\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m myPipeline=\u001b[43mEvaluationPipeline\u001b[49m(proposed_solution_path=AI_comp_path, \u001b[38;5;66;03m# Set here the AI component path you want to evaluate\u001b[39;00m\n\u001b[32m      6\u001b[39m                               meta_root_path=\u001b[33m\"\u001b[39m\u001b[33mstarter_kit_pipeline_results\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;66;03m# Set the directory here where pipeline results will be stored (inference results, and computed metrics)\u001b[39;00m\n\u001b[32m      7\u001b[39m                               cache_strategy=\u001b[33m\"\u001b[39m\u001b[33mlocal\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;66;03m# \"local\" or \"remote\" .If set on \"local\", all image used for evaluation , will be locally stored in a cache directory. Else, image will be loaded directly from downloding\u001b[39;00m\n\u001b[32m      8\u001b[39m                               cache_dir=\u001b[33m\"\u001b[39m\u001b[33mevaluation_cache\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;66;03m# chosen directory for cache\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'EvaluationPipeline' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define path of AI component to test, \n",
    "AI_comp_path= \"..\\\\reference-solutions\\\\Solution-1\"\n",
    "\n",
    "# Initialize test pipeline\n",
    "myPipeline=EvaluationPipeline(proposed_solution_path=AI_comp_path, # Set here the AI component path you want to evaluate\n",
    "                              meta_root_path=\"starter_kit_pipeline_results\", # Set the directory here where pipeline results will be stored (inference results, and computed metrics)\n",
    "                              cache_strategy=\"local\", # \"local\" or \"remote\" .If set on \"local\", all image used for evaluation , will be locally stored in a cache directory. Else, image will be loaded directly from downloding\n",
    "                              cache_dir=\"evaluation_cache\") # chosen directory for cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fddaef-6902-4025-b785-2e3ee76aa7a9",
   "metadata": {},
   "source": [
    "## Load your AI component into the evaluation environnement\n",
    "\n",
    "The load_proposed_solution() method below is divided into two main tasks:\n",
    "- Install the python package of your AI component --> ( execute the commande pip install AI_comp_path)\n",
    "- Call the load_model() method of your AIcomponent interface\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75924591-27d3-47b3-b6e1-7bf423321581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI component loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\SAUVEGARDES_FIN_CONFIANCE\\CSIA++\\Challenge\\Build2025\\env-test-v2-starter-kit\\Lib\\site-packages\\keras\\src\\layers\\preprocessing\\tf_data_layer.py:19: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "myPipeline.load_proposed_solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd65bfb-9bf1-487b-a752-b82df711d9a6",
   "metadata": {},
   "source": [
    "## Load an evaluation dataset metadescription\n",
    "\n",
    "In the next cell, we load the metadata of the evaluation dataset we want to use to evaluate our AI component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2a83a8a-c53d-42cf-8bda-feccbd9fba45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://minio-storage.apps.confianceai-public.irtsysx.fr/challenge-welding/datasets/example_mini_dataset/metadata/ds_meta.parquet\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>class</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>welding-seams</th>\n",
       "      <th>labelling_type</th>\n",
       "      <th>resolution</th>\n",
       "      <th>path</th>\n",
       "      <th>sha256</th>\n",
       "      <th>storage_type</th>\n",
       "      <th>data_origin</th>\n",
       "      <th>blur_level</th>\n",
       "      <th>blur_class</th>\n",
       "      <th>luminosity_level</th>\n",
       "      <th>external_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data_92409</td>\n",
       "      <td>OK</td>\n",
       "      <td>22/01/20 12:49</td>\n",
       "      <td>c33</td>\n",
       "      <td>expert</td>\n",
       "      <td>[1920, 1080]</td>\n",
       "      <td>challenge-welding/datasets/example_mini_datase...</td>\n",
       "      <td>b'GN\\xd7\\xa7B\\x98\\xb0r\\xa4\\xdfn\\x8cT\\x8e:\\xc07...</td>\n",
       "      <td>s3</td>\n",
       "      <td>real</td>\n",
       "      <td>701.938341</td>\n",
       "      <td>blur</td>\n",
       "      <td>50.533365</td>\n",
       "      <td>http://minio-storage.apps.confianceai-public.i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data_67943</td>\n",
       "      <td>OK</td>\n",
       "      <td>20/02/20 23:53</td>\n",
       "      <td>c102</td>\n",
       "      <td>expert</td>\n",
       "      <td>[1920, 1080]</td>\n",
       "      <td>challenge-welding/datasets/example_mini_datase...</td>\n",
       "      <td>b's\\xf6;3i-\\x10\\xfd8y\\xf2\\xe1\\xa6JQ\\x84`\\xc6\\x...</td>\n",
       "      <td>s3</td>\n",
       "      <td>real</td>\n",
       "      <td>715.670702</td>\n",
       "      <td>blur</td>\n",
       "      <td>47.050604</td>\n",
       "      <td>http://minio-storage.apps.confianceai-public.i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data_4843</td>\n",
       "      <td>OK</td>\n",
       "      <td>20/01/20 20:34</td>\n",
       "      <td>c20</td>\n",
       "      <td>expert</td>\n",
       "      <td>[1920, 1080]</td>\n",
       "      <td>challenge-welding/datasets/example_mini_datase...</td>\n",
       "      <td>b'\\xdbZ\\xb3\\x12e&amp;\\xd5\\x83\\x13*\\x87S\\xe1\\x19\\xc...</td>\n",
       "      <td>s3</td>\n",
       "      <td>real</td>\n",
       "      <td>715.857380</td>\n",
       "      <td>blur</td>\n",
       "      <td>46.204245</td>\n",
       "      <td>http://minio-storage.apps.confianceai-public.i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data_25309</td>\n",
       "      <td>OK</td>\n",
       "      <td>18/07/2022 20:18</td>\n",
       "      <td>c102</td>\n",
       "      <td>operator</td>\n",
       "      <td>[960, 540]</td>\n",
       "      <td>challenge-welding/datasets/example_mini_datase...</td>\n",
       "      <td>b'/c\\xe3\\xd9\\xc8|&amp;\\xaf\\xb1}\\xf6\\xe3s\\xae\\xea\\x...</td>\n",
       "      <td>s3</td>\n",
       "      <td>real</td>\n",
       "      <td>869.513006</td>\n",
       "      <td>blur</td>\n",
       "      <td>34.359280</td>\n",
       "      <td>http://minio-storage.apps.confianceai-public.i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data_76144</td>\n",
       "      <td>OK</td>\n",
       "      <td>03/10/19 21:14</td>\n",
       "      <td>c20</td>\n",
       "      <td>expert</td>\n",
       "      <td>[1920, 1080]</td>\n",
       "      <td>challenge-welding/datasets/example_mini_datase...</td>\n",
       "      <td>b'\\xca%\\x0c\\x92\\x1f\\x0c\\x00\\xcc\\x02\\r\\xb8\\xf1\\...</td>\n",
       "      <td>s3</td>\n",
       "      <td>real</td>\n",
       "      <td>2676.246904</td>\n",
       "      <td>clean</td>\n",
       "      <td>46.256244</td>\n",
       "      <td>http://minio-storage.apps.confianceai-public.i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sample_id class         timestamp welding-seams labelling_type  \\\n",
       "0  data_92409    OK    22/01/20 12:49           c33         expert   \n",
       "1  data_67943    OK    20/02/20 23:53          c102         expert   \n",
       "2   data_4843    OK    20/01/20 20:34           c20         expert   \n",
       "3  data_25309    OK  18/07/2022 20:18          c102       operator   \n",
       "4  data_76144    OK    03/10/19 21:14           c20         expert   \n",
       "\n",
       "     resolution                                               path  \\\n",
       "0  [1920, 1080]  challenge-welding/datasets/example_mini_datase...   \n",
       "1  [1920, 1080]  challenge-welding/datasets/example_mini_datase...   \n",
       "2  [1920, 1080]  challenge-welding/datasets/example_mini_datase...   \n",
       "3    [960, 540]  challenge-welding/datasets/example_mini_datase...   \n",
       "4  [1920, 1080]  challenge-welding/datasets/example_mini_datase...   \n",
       "\n",
       "                                              sha256 storage_type data_origin  \\\n",
       "0  b'GN\\xd7\\xa7B\\x98\\xb0r\\xa4\\xdfn\\x8cT\\x8e:\\xc07...           s3        real   \n",
       "1  b's\\xf6;3i-\\x10\\xfd8y\\xf2\\xe1\\xa6JQ\\x84`\\xc6\\x...           s3        real   \n",
       "2  b'\\xdbZ\\xb3\\x12e&\\xd5\\x83\\x13*\\x87S\\xe1\\x19\\xc...           s3        real   \n",
       "3  b'/c\\xe3\\xd9\\xc8|&\\xaf\\xb1}\\xf6\\xe3s\\xae\\xea\\x...           s3        real   \n",
       "4  b'\\xca%\\x0c\\x92\\x1f\\x0c\\x00\\xcc\\x02\\r\\xb8\\xf1\\...           s3        real   \n",
       "\n",
       "    blur_level blur_class  luminosity_level  \\\n",
       "0   701.938341       blur         50.533365   \n",
       "1   715.670702       blur         47.050604   \n",
       "2   715.857380       blur         46.204245   \n",
       "3   869.513006       blur         34.359280   \n",
       "4  2676.246904      clean         46.256244   \n",
       "\n",
       "                                       external_path  \n",
       "0  http://minio-storage.apps.confianceai-public.i...  \n",
       "1  http://minio-storage.apps.confianceai-public.i...  \n",
       "2  http://minio-storage.apps.confianceai-public.i...  \n",
       "3  http://minio-storage.apps.confianceai-public.i...  \n",
       "4  http://minio-storage.apps.confianceai-public.i...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# In this example we will choose a small dataset\n",
    "\n",
    "ds_name=\"example_mini_dataset\"\n",
    "\n",
    "# Load all metadata of your dataset as a pandas dataframe, (you can point to a local cache metafile instead of original one pointing on remote repository)\n",
    "\n",
    "my_challenge_UI=ChallengeUI()\n",
    "evaluation_ds_meta_df=my_challenge_UI.get_ds_metadata_dataframe(ds_name)\n",
    "\n",
    "display(evaluation_ds_meta_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e70108-edb7-4de7-85ab-8048a737aaa0",
   "metadata": {},
   "source": [
    "##  Perform inference on an evaluation dataset\n",
    "\n",
    "We pass the evaluation_dataframe in the method below. It use the loaded AI component to perform inference of each sample referenced in the evaluation dataframe and the inference results as new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02df7d85-68dc-4f3f-8456-91758c06f5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of  batch to process for inference :  20  , start processing..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█████████████████████████▊                                                                                                                                                  | 3/20 [00:05<00:31,  1.85s/it]"
     ]
    }
   ],
   "source": [
    "result_df=myPipeline.perform_grouped_inference(evaluation_dataset=evaluation_ds_meta_df, # dataframe containing metadescription of your evaluation ds\n",
    "                                               results_inference_path=myPipeline.meta_root_path+\"/res_inference.parquet\", # path to file that will contains inference_results\n",
    "                                               batch_size=150 # You can group inference by batch if you want\n",
    "                                              ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e12ffb1-1182-4e6d-b23c-578231d12199",
   "metadata": {},
   "source": [
    "You can see the inference results below. See that new column has been added :\n",
    "- \"predicted_state\" imported from \"predicitions\" key of your AI component predict method output dictionnary\n",
    "- \"scores KO\" imported from \"probabilities\" key of your AI component predict method output dictionnary\n",
    "- \"scores OK\" imported from \"probabilities\" key of your AI component predict method output dictionnary\n",
    "- \"score OOD\" imported from \"OOD scores\" key of your AI component predict method output dictionnary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c18ec49-b6ce-413e-b209-b75634a44193",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78948c8f-361c-475f-ad3a-729022f181e6",
   "metadata": {},
   "source": [
    "## Compute operationnal metrics\n",
    "\n",
    "The first criterion to evaluate your component quality will be the operationnal cost. The objective is to compare the cost of using your AI component versus\n",
    "the cost of using only humans to process images from the evaluation dataset. The import scores to maximimze is \"gain in euros\" and inference_time\n",
    "\n",
    "As the example AI component we provided in this starter-kit has not been designed to be performant, here the gain_score is very bad\n",
    "\n",
    "The below method compute these two metrics, results are display as output, and in results folder defined at this pipeline initilization too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfc2c1b-0f24-4e50-80fd-8afb26187fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute operationnal metrics\n",
    "myPipeline.compute_operationnal_metrics(AIcomp_name=\"sol_0\", # This name is only used for the name of result files\n",
    "                                        res_inference_path=myPipeline.meta_root_path+\"/res_inference.parquet\" # inference_results file\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880c4cd0-5234-4948-bdd3-bd0e66e4b877",
   "metadata": {},
   "source": [
    "## Compute uncertainty metrics\n",
    "\n",
    "Same thing with uncertainty metrics\n",
    "\n",
    "To detail\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c516ad12-2af2-45aa-a9ed-69c69c38d269",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df,final_results=myPipeline.compute_uncertainty_metrics(res_inference_path=myPipeline.meta_root_path+\"/res_inference.parquet\",\n",
    "                                                            AIcomp_name=\"sol_0\"\n",
    "                                                            )            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05222ca9-1222-4919-b3b9-b7c07c2f3b07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-test-starter-kit",
   "language": "python",
   "name": "env-test-starter-kit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
